{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6acacb-21b4-455d-b99b-2fbba20602c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import regex as re\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from xmltodict import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from xmltodict import parse\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a032fd8-5149-4811-94d7-7bcb6e6b42a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HW3\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "### Q1\n",
    "\n",
    "1. Modify the code I wrote in lecture 8 with what you have learnt in lecture 9 and correctly tokenize the text both on the word and sentence level, and by removing the stopwords. Rewrite the `getSummary` function and all the other functions that it depends by maing these corrections.\n",
    "\n",
    "2. Rewrite the code I wrote for `getKeywords` function making the same corrections.\n",
    "\n",
    "3. Test your code from parts 1 and 2 on random articles from the Guardian.\n",
    "\n",
    "4. Rewrite the `getSubjectGuardian` function for another newspaper in English, and test your code from part 1 and 2 on random articles from this new newspaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23c1357c-12b0-412b-8c88-e6867691551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.get('https://www.theguardian.com/film/rss') as link:\n",
    "    raw = parse(link.text)\n",
    "nal = raw['rss']['channel']['item']\n",
    "def getSubjectGuardian(subject):\n",
    "    with requests.get(f'https://www.theguardian.com/{subject}/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "nba = getSubjectGuardian('sport/nba')\n",
    "film = getSubjectGuardian('film')\n",
    "fashion = getSubjectGuardian('fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5cf744-59e4-410f-aef4-23390472f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba[0]['link']\n",
    "with requests.get(nba[0]['link']) as link:\n",
    "    raw = BeautifulSoup(link.content,'html.parser')\n",
    "swEN = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68da161-2f8c-4873-b00d-491cf59820d8",
   "metadata": {},
   "source": [
    "Q1.1 : We are rewriting functions with corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7bea7e-d036-447e-8f64-1dc6a2286cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f712f2d-7b03-4ac3-9983-14d5c81939ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text):\n",
    "    sentences = {'sentences': sent_tokenize(text)}\n",
    "    sentences.update({'cleanedSentences': [re.sub(r'[^\\p{Letter}\\s]','',sentence.lower()) for sentence in sentences['sentences']]})\n",
    "    return [re.sub(r'[^\\w\\s]','',x.lower()) for x in sentences['cleanedSentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "999460cf-a6fd-460a-949f-42741e08cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(sentences):\n",
    "    vectorizer = CountVectorizer(stop_words=swEN)\n",
    "    return vectorizer.fit_transform(sentences)\n",
    "\n",
    "def getSummary(text,k):\n",
    "    sentences = processText(text)\n",
    "    matrix = getMatrix(sentences)\n",
    "    projection = PCA(n_components=1)\n",
    "    weights = projection.fit_transform(matrix.toarray())\n",
    "    res = list(zip(weights.transpose()[0],range(112),sentences))\n",
    "    tmp = sorted(res,key=lambda x: x[0],reverse=True)[:k]\n",
    "    return sorted(tmp, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6b099-a016-412e-ae1e-b41fc8492172",
   "metadata": {},
   "source": [
    "Q1.2: Tokenized and removed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a7f00d-592f-4597-b3b6-e545b8d48bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeywords(text,sw,k):\n",
    "    ex = {'sentences': sent_tokenize(text)}\n",
    "    ex.update({'cleanedSentences': [re.sub(r'[^\\p{Letter}\\s]','',sentence.lower()) for sentence in ex['sentences']]})     \n",
    "    vectorizer = CountVectorizer(stop_words=sw)\n",
    "    matrix = vectorizer.fit_transform(ex['cleanedSentences'])\n",
    "    words = vectorizer.get_feature_names()\n",
    "    projection = PCA(n_components=1)\n",
    "    tmp = projection.fit_transform(matrix.transpose().toarray())\n",
    "    weights = tmp.transpose()[0]\n",
    "    return sorted(zip(weights,words),key=lambda x: x[0], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773eb599-4630-4547-b565-f75635173cd2",
   "metadata": {},
   "source": [
    "Q1.3: Testing our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d69a08-ebc2-47b0-8fcc-124d38bcacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.977206769207988, 0, 'the queer eye presenters look into skin bleaching and colourism sees him sensitively interview kelly rowland listen to abusive tweets  and reveal his own heartrending past when tan france was nine he stole his cousins skin bleaching cream'), (2.45919096152794, 3, 'the shame and guilt that queer eyes resident fashion expert feels about his youthful pursuit of lighter skin is the ostensible motive behind tan france beauty and the bleach bbc two a celebrityodyssey documentary that starts as is now custom with the obligatory suitcasepacking shot'), (2.1929839477887567, 4, 'the doncasterraised stylist is leaving his deluxe utah home and returning to mustyfusty old england to reckon with his feelings about skin bleaching and investigate the wider problem of colourism the idea that fairer complexions are favoured in black and asian communities as well as society at large')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2.3761318802671063, 'skin'),\n",
       " (1.8739540399901167, 'bleaching'),\n",
       " (1.2256589189685063, 'france')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(1, 10)\n",
    "text=getText(fashion[i]['link'])\n",
    "\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd217477-e749-44d9-bdde-37052622d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.4596752430077338, 6, 'somebody get me in contact with the young lady'), (0.5297234201070614, 9, 'so thats definitely unacceptable on my part and i take full responsibility'), (4.024848996021901, 12, 'so that was definitely wrong a lot of emotions and hopefully i can get in contact with the young girl and sincerely apologize to her and do something nice for her but thats definitely on me he added that he wouldnt fight any punishment handed down by the nba')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.7940987603897578, 'definitely'),\n",
       " (1.0564636452389606, 'get'),\n",
       " (1.0131352889221652, 'contact')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(1, 10)\n",
    "text=getText(nba[i]['link'])\n",
    "\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ad70c-0a4c-46c5-a8ad-a753b3710606",
   "metadata": {},
   "source": [
    "Q1.4: Testing with different newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cac6a5-1af1-4025-9267-5b83a841c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectNews(subject):\n",
    "    with requests.get(f'http://en.people.cn/rss/{subject}.xml') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "china = getSubjectNews('China')\n",
    "business = getSubjectNews('Business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35543926-bf63-46e1-bacf-7aee4bbb5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-8.760707733967617, 0, '\\n \\n\\n\\n cpc\\xa0newschina\\xa0tibet\\xa0onlineforum \\nsina microblog\\ntencent microblog\\nfeedback\\nrss\\n editor张茜 \\r\\n                      \\xa0\\xa0\\xa0\\xa0\\n 双语词典dictionary besides mars china plans deep space tracking   best space pictures of   giant christmas ornament seen in space in time for holiday  chinas space industry to achieve leapforward development  us astronomers discover buckyballs in space  china to probe deep space  china made  breakthroughs in space telemetry tracking control  light and space create tranquility  interview china contributes to spacebased information access a lot un official  magnificent space photos in june    tibet poised to embrace even brighter future  years after peaceful liberation thunderstorms forecast to continue in north china chinese tourism official confident of crossstrait travel anger over obama meeting dalai lama chinese official calls for more language culture exchanges with foreign countries senior chinese leader calls for efforts to develop new energy central govt delegation arrives in lhasa for tibet peaceful liberation celebrations vice president cuts ribbons for tibets first expressway death toll rises to  in ne china road accident china southern airlines sends charter flight carrying peacekeepers to liberia  china train crash new tragedy for hispeed rail repatriation of lai shows resolution to punish crimes\\n hillarys strategic stake might irk beijing y transport aircraft undergoing test flight'), (8.76070773396764, 1, 'norway massacre peace country not peaceful any more \\r\\nlibya seeks talks as nato strikes hit capital us next step mars rover china\\xa0daily\\xa0global\\xa0times\\xa0cctv\\xa0chinaorgcn\\xa0cri\\xa0eastday\\xa0beijing\\xa0review\\xa0ebeijingchina\\xa0today\\xa0china\\xa0travel\\xa0china\\xa0travel\\xa0designer')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7.977861796943847, 'space'),\n",
       " (6.884773045137203, 'china'),\n",
       " (1.9807812882548177, 'chinese')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(1, 10)\n",
    "text=getText(china[i]['link'])\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d48d02-8ae0-43b3-9588-a5c5e2317d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.218285461092141, 0, 'set as homepageregistersign inchinesebig frenchrussianspanishjapanesearabic special coveragepd online databasesina microblogtencent microblogfeedback newsopinionschina militaryforeign affairslearn chinesechina studyforum edited and translated by peoples daily online according to data on every countrys gold reserves recently issued by world gold council wgc the united states gold reserves are  tons accounting for  percent of the worlds total and the untied states is still the largest gold reserve country'), (2.2260750504207176, 3, 'in comparison the untied states gold reserves account for  percent of its foreign reserve and even emerging countries including russia and india have gold reserves accounting for more than  percent of their foreign reserves'), (2.9840211287671834, 6, 'the gold purchased by every country in  is three times as much as the gold they purchased in in fact before standard and poors downgraded the us credit rating the central bank of every country already started to gradually increase their gold reserves because of the european and us debt crises and the declining confidence in the us dollar and euro')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9.36416921736123, 'gold'),\n",
       " (5.573584647458165, 'reserves'),\n",
       " (1.7843895533955678, 'foreign')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=getText(business[0]['link'])\n",
    "\n",
    "print(getSummary(text,3))\n",
    "getKeywords(text,swEN,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed212f-6c1c-4239-96b4-cee3538c135b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q2\n",
    "\n",
    "Write a function that returns all named entities (proper names, country names, corporation names only) from a URL. Function should take the URL as the input and must return the list of named entities from that URL. Test your code on random articles from the Guardian. Don't use the NLTK's NER that I demonstrated during the lecture. Use the SpaCY's NER function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f199d-fcba-4207-af09-1b443dafb653",
   "metadata": {},
   "source": [
    "We wrote the function with the NER function of the spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2862394-21eb-432d-9f3c-f3e4006d5323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trae Young', 'Atlanta Hawks', 'Clint Capela', 'the Cleveland Cavaliers', 'the Eastern Conference’s', 'Hawks', 'Cavs', 'Bogdan Bogdanovic', 'Atlanta', 'Miami', 'South Florida', 'Hawks', 'Capela', 'Lauri Markkanen', 'Cleveland', 'Cavs', 'LeBron James', 'James', 'Hawks', 'Cleveland', 'Rocket Mortgage Fieldhouse', 'Cavs', 'Cleveland', 'Cavs', 'Young', 'Cleveland', 'Cavs', 'James', 'NBA', 'Cleveland', 'Cleveland', 'Jarrett Allen', 'Allen', 'Hawks', 'Young', 'Cleveland', 'Atlanta', 'Hawks', 'Markkanen', 'Cleveland', 'Atlanta', 'Brandon Ingram', 'Los Angeles', 'Willie Green', 'New Orleans', 'Green', 'Clipper', 'Zion Williamson', 'Trey Murphy', 'CJ McCollum', 'Larry Nance Jr.', 'New Orleans', 'Nance', 'McCollum', 'Portland', 'Paul George', 'Kawhi Leonard', 'Marcus Morris', 'Reggie Jackson', 'Clippers', 'Pelicans', 'Jackson', 'Morris', 'Robert Covington', 'Ivica Zubac', 'Ingram', 'Pelicans', 'New Orleans', 'McCollum', 'Murphy', 'Murphy', 'Clippers', 'Jackson', 'Morris', 'Clippers', 'Luke Kennard', 'Minnesota']\n"
     ]
    }
   ],
   "source": [
    "def function(url):\n",
    "    with requests.get(f'https://www.theguardian.com/sport/nba/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "        raw1= raw['rss']['channel']['item']\n",
    "        i = random.randint(1, 10)\n",
    "    with requests.get(raw1[i]['link']) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "        a=' '.join([x.text for x in raw.find_all('p')])\n",
    "        ner = spacy.load(\"en_core_web_sm\")\n",
    "        a1 = ner(a)\n",
    "        print([i.text for i in a1.ents if i.label_ in ('GPE','ORG','PERSON')])\n",
    "function(f'https://www.theguardian.com/nba/rss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf558c97-c917-4735-bed3-f46c0c1d23da",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "1. Write a function that returns the most positive and the most negative sentences from a text. The function must take the text as the input and must return a 2-tuple: the first element as the most positive and the second as the most negative sentence with their polarity scores.\n",
    "\n",
    "2. Test your function on random articles from the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7dd84c2-2343-488b-af0b-88fb7597958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbde03-a955-4cde-8140-62cb2620886f",
   "metadata": {},
   "source": [
    "The function first takes the text and splits it into sentences. Then it takes the compound score of each sentence into a list together with the sentence. Then it converts this list as a pandas dataframe and ranks the compound scores from smallest to largest. It takes the largest and smallest data as a new pandas dataframe. It turns that dataframe into a list of 2 tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ccc4661-a0be-4041-9774-bc53f8c16590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textanalyser(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = []\n",
    "    for number, sentence in enumerate(sentences):\n",
    "        scores = analyser.polarity_scores(sentence)\n",
    "        sentence_scores.append({'sentence': sentence, 'sentence_number': number+1, 'sentiment_score': scores['compound']})\n",
    "    df = pd.DataFrame(sentence_scores)\n",
    "    df = df.sort_values(by='sentiment_score')\n",
    "    df2 = df.iloc[[0, -1]]\n",
    "    result = list(df2.itertuples(index=False, name=None))\n",
    "    neg = result[0]\n",
    "    pos = result[1]\n",
    "    return neg,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "468aabcf-9151-4545-aba5-a915a942d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Trae Young scored 32 of his 38 points in the second half and the Atlanta Hawks overcame losing center Clint Capela to a knee injury to beat the Cleveland Cavaliers 107-101 in the play-in on Friday night and earn the Eastern Conference’s No 8 playoff seed.',\n",
       "  1,\n",
       "  -0.765),\n",
       " ('I’ve been in seasons like that, it’s hard to keep going and keep striving for it.” The Clippers were dealt a huge blow earlier in the day when Paul George entered the league’s health and safety protocols.',\n",
       "  42,\n",
       "  0.7351))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(1, 10)\n",
    "textanalyser(getText(nba[i]['link']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92567b61-c5e6-434f-993d-3129fcf81ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
